{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57d88cbd",
   "metadata": {},
   "source": [
    "## Outline of the program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a751c5a",
   "metadata": {},
   "source": [
    "## We will define various functions according to the task we want to perform.\n",
    "\n",
    "### The tasks are as follows:\n",
    "\n",
    "\n",
    "#### Step 1)  Importing the excel file containing the list of urls\n",
    "#### Step 2)  Iterating through the urls, we have to scrape the main article text from the webpage of given url.\n",
    "#### step 3)  Clean the the scraped text by removing HTML tags\n",
    "#### step 4)  Using Natural Language Processing, tokeninze the text.\n",
    "#### step 5)  Remove the stopwords with the help of given list of stopwords.\n",
    "#### step 6)  Sentiment analysis by comparing the text with master dictionary given.\n",
    "#### step 7)  Calculating the score for various parameters asked.\n",
    "#### step 8)  Writing the output one by one in the given format in an excel sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20a4f06",
   "metadata": {},
   "source": [
    "## Given below is the list of functions with their definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4457e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                          # Importing pandas library\n",
    "import requests                                              # Importing requests library\n",
    "from bs4 import BeautifulSoup                               # Importing BeautifulSoup from bs4\n",
    "import spacy                                                 # Importing spacy library\n",
    "from textblob import TextBlob                               # Importing TextBlob from textblob\n",
    "import syllapy                                              # Importing syllapy\n",
    "import os\n",
    "\n",
    "\n",
    "def url_links(file_path):\n",
    "    \"\"\"\n",
    "    Reads the URLs from an Excel file.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The path to the Excel file containing URLs.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: DataFrame containing URLs.\n",
    "    \"\"\"\n",
    "    return pd.read_excel(file_path)                          # Reading URLs from an Excel file and returning DataFrame\n",
    "\n",
    "\n",
    "file_path = 'F:/Blackcoffer assignment/input.xlsx'\n",
    "positive_words = 'F:/Blackcoffer assignment/MasterDictionary-20230812T205219Z-001/MasterDictionary/positive-words.txt'\n",
    "negative_words = 'F:/Blackcoffer assignment/MasterDictionary-20230812T205219Z-001/MasterDictionary/negative-words.txt'\n",
    "output_file = 'F:/Blackcoffer assignment/output.xlsx'\n",
    "\n",
    "\n",
    "\n",
    "def merge_stopword_files(folder_path, All_stop_words_path):\n",
    "   \n",
    "    stopword_files = []   \n",
    "    for filename in os.listdir(folder_path):      # Iterate over the files in the folder        \n",
    "        if filename.endswith('.txt'):             # Check if the file is a text file\n",
    "            stopword_files.append(os.path.join(folder_path, filename))      # Append the full path of the file to the list \n",
    "    with open(All_stop_words_path, 'w') as AllStopWords:     # Open the new file in write mode  \n",
    "        for file_path in stopword_files:    # Iterate through each file in the stopword_files list          \n",
    "            with open(file_path, 'r') as file:  # Open each file in read mode             \n",
    "                # Read the contents of the file\n",
    "                content = file.read()\n",
    "                # Write the contents to the new file\n",
    "                AllStopWords.write(content)\n",
    "                # Add a newline character to separate contents from different files\n",
    "                AllStopWords.write('\\n')\n",
    "# Specify the folder path containing stop word files\n",
    "folder_path = 'F:/Blackcoffer assignment/StopWords-20230812T205218Z-001/StopWords'\n",
    "# Define the path of the new file\n",
    "All_stop_words_path = 'F:/Blackcoffer assignment/StopWords-20230812T205218Z-001/StopWords/AllStopWords.txt'\n",
    "# Call the function to merge stop word files\n",
    "merge_stopword_files(folder_path, All_stop_words_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scrape_text(url):\n",
    "    \"\"\"\n",
    "    Scrapes the text content from a given URL.\n",
    "\n",
    "    Args:\n",
    "    - url (str): The URL of the webpage to scrape.\n",
    "\n",
    "    Returns:\n",
    "    - list: List containing text content scraped from the webpage.\n",
    "    \"\"\"\n",
    "    article_text = requests.get(url).text                    # Getting text content from the URL\n",
    "    soup = BeautifulSoup(article_text, 'lxml')               # Creating BeautifulSoup object\n",
    "    article = soup.find_all('div', class_='td-post-content tagdiv-type')    # Finding text content with specific class\n",
    "    if not article:\n",
    "        article = soup.find_all('div', class_='tdb-block-inner td-fix-index')   # Finding text content with specific class if the first search fails\n",
    "    return article             # Returning the scraped text content\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_the_text(article):\n",
    "    \"\"\"\n",
    "    Cleans the extracted text content. i.e. removing the html tags.\n",
    "\n",
    "    Args:\n",
    "    - article (list): List containing text content scraped from the webpage.\n",
    "\n",
    "    Returns:\n",
    "    - str: Cleaned text content.\n",
    "    \"\"\"\n",
    "    cleaned_article = ''                                     # Initializing an empty string for cleaned text content\n",
    "    for block in article:\n",
    "        block_text = block.get_text(strip=True)              # Getting text content without extra spaces\n",
    "        cleaned_article += block_text.strip()                # Adding the cleaned text content to the string\n",
    "        cleaned_article = cleaned_article.replace('\\n', ' ')  # Replacing newline characters with spaces\n",
    "    return cleaned_article                                   # Returning the cleaned text content\n",
    "\n",
    "def tokenize_article(cleaned_article, stopword_file):\n",
    "    \"\"\"\n",
    "    Tokenizes the cleaned text content.\n",
    "\n",
    "    Args:\n",
    "    - cleaned_article (str): Cleaned text content.\n",
    "    - stopword_file (str): Path to the file containing stop words.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of filtered tokens.\n",
    "    \"\"\"\n",
    "    with open(stopword_file, 'r') as stopwords_file:         # Opening the stop words file\n",
    "        stopwords = stopwords_file.read().splitlines()       # Reading stop words and splitting by lines\n",
    "        \n",
    "    nlp = spacy.load('en_core_web_lg')                       # Loading the English language model\n",
    "    doc = nlp(cleaned_article)                               # Creating a document object with Spacy\n",
    "    filtered_tokens = [token.text.lower() for token in doc if token.text.lower() not in stopwords]    # Filtering tokens based on stop words\n",
    "    return filtered_tokens                                   # Returning the filtered tokens\n",
    "\n",
    "def calculate_scores(filtered_tokens, positive_words_file, negative_words_file):\n",
    "    \"\"\"\n",
    "    Calculates sentiment scores.\n",
    "\n",
    "    Args:\n",
    "    - filtered_tokens (list): List of filtered tokens.\n",
    "    - positive_words_file (str): Path to the file containing positive words.\n",
    "    - negative_words_file (str): Path to the file containing negative words.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Tuple containing positive score, negative score, polarity score, and subjectivity score.\n",
    "    \"\"\"\n",
    "    with open(positive_words_file, 'r', encoding='latin-1') as pwords:   # Opening positive words file\n",
    "        positive_words = set(pwords.read().splitlines())         # Reading positive words and converting to set\n",
    "        \n",
    "    with open(negative_words_file, 'r', encoding='latin-1') as nwords:   # Opening negative words file\n",
    "        negative_words = set(nwords.read().splitlines())         # Reading negative words and converting to set\n",
    "        \n",
    "    positive_score = sum(1 for word in filtered_tokens if word in positive_words)    # Calculating positive score\n",
    "    negative_score = sum(1 for word in filtered_tokens if word in negative_words)    # Calculating negative score\n",
    "    polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 1e-10)    # Calculating polarity score\n",
    "    subjectivity_score = TextBlob(' '.join(filtered_tokens)).sentiment.subjectivity    # Calculating subjectivity score\n",
    "    return positive_score, negative_score, polarity_score, subjectivity_score    # Returning scores\n",
    "\n",
    "def analyze_text(filtered_tokens):\n",
    "    \"\"\"\n",
    "    Analyzes the text content.\n",
    "\n",
    "    Args:\n",
    "    - filtered_tokens (list): List of filtered tokens.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Tuple containing various analysis results.\n",
    "    \"\"\"\n",
    "    num_sentences = len(TextBlob(' '.join(filtered_tokens)).sentences)    # Getting the number of sentences\n",
    "    \n",
    "    # Check if filtered_tokens is empty\n",
    "    if not filtered_tokens:\n",
    "        # Return zeros for all analysis results\n",
    "        return 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    \n",
    "    avg_sentence_length = len(filtered_tokens) / num_sentences    # Calculating average sentence length\n",
    "    percentage_complex_words = sum(1 for word in filtered_tokens if len(word) > 6) / len(filtered_tokens)    # Calculating percentage of complex words\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)    # Calculating FOG index\n",
    "    avg_num_words_per_sentence = len(filtered_tokens) / num_sentences    # Calculating average number of words per sentence\n",
    "    complex_word_count = sum(1 for word in filtered_tokens if len(word) > 6)    # Calculating count of complex words\n",
    "    word_count = len(filtered_tokens)    # Getting the total word count\n",
    "    syllables_per_word = sum(syllapy.count(word) for word in filtered_tokens) / len(filtered_tokens)    # Calculating average syllables per word\n",
    "    personal_pronouns = sum(1 for word in filtered_tokens if word.lower() in {'i', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves'})    # Calculating count of personal pronouns\n",
    "    avg_word_length = sum(len(word) for word in filtered_tokens) / len(filtered_tokens)    # Calculating average word length\n",
    "    return avg_sentence_length, percentage_complex_words, fog_index, avg_num_words_per_sentence, complex_word_count, word_count, syllables_per_word, personal_pronouns, avg_word_length    # Returning analysis results\n",
    "\n",
    "def process_urls(input_file_path, output_file_path, All_stop_words_path, positive_words_file, negative_words_file):\n",
    "    \"\"\"\n",
    "    Processes URLs and saves results to an output file.\n",
    "\n",
    "    Args:\n",
    "    - input_file_path (str): Path to the input Excel file containing URLs.\n",
    "    - output_file_path (str): Path to save the output Excel file.\n",
    "    - stopword_file_path (str): Path to the file containing stop words.\n",
    "    - positive_words_file (str): Path to the file containing positive words.\n",
    "    - negative_words_file (str): Path to the file containing negative words.\n",
    "    \"\"\"\n",
    "    input_df = url_links(input_file_path)    # Reading input Excel file\n",
    "    output_df = pd.DataFrame(columns=['URL_ID', 'URL', 'Positive Score', 'Negative Score', 'Polarity Score', 'Subjectivity Score', 'Avg Sentence Length', 'Percentage of Complex Words', 'FOG Index', 'Avg Number of Words per Sentence', 'Complex Word Count', 'Word Count', 'Syllable per Word', 'Personal Pronouns', 'Avg Word Length'])    # Creating an empty DataFrame for output\n",
    "\n",
    "    for index, row in input_df.iterrows():    # Iterating through rows of input DataFrame\n",
    "        url_id=row['URL_ID']\n",
    "        url = row['URL']    # Extracting URL from DataFrame\n",
    "        article_text = scrape_text(url)    # Scraping text content from the URL\n",
    "        cleaned_text = clean_the_text(article_text)    # Cleaning the extracted text content\n",
    "        filtered_tokens = tokenize_article(cleaned_text, All_stop_words_path)    # Tokenizing the cleaned text content\n",
    "        positive_score, negative_score, polarity_score, subjectivity_score = calculate_scores(filtered_tokens, positive_words_file, negative_words_file)    # Calculating sentiment scores\n",
    "        analysis_results = analyze_text(filtered_tokens)    # Analyzing the text content\n",
    "\n",
    "        # Add results to output DataFrame\n",
    "        row_data = {\n",
    "            'URL_ID': url_id,\n",
    "            'URL': url,\n",
    "            'Positive Score': positive_score,\n",
    "            'Negative Score': negative_score,\n",
    "            'Polarity Score': polarity_score,\n",
    "            'Subjectivity Score': subjectivity_score,\n",
    "            'Avg Sentence Length': analysis_results[0],\n",
    "            'Percentage of Complex Words': analysis_results[1],\n",
    "            'FOG Index': analysis_results[2],\n",
    "            'Avg Number of Words per Sentence': analysis_results[3],\n",
    "            'Complex Word Count': analysis_results[4],\n",
    "            'Word Count': analysis_results[5],\n",
    "            'Syllable per Word': analysis_results[6],\n",
    "            'Personal Pronouns': analysis_results[7],\n",
    "            'Avg Word Length': analysis_results[8]\n",
    "        }\n",
    "        output_df = pd.concat([output_df, pd.DataFrame([row_data])], ignore_index=True)    # Concatenating row data to output DataFrame\n",
    "\n",
    "    # Save output DataFrame to Excel file\n",
    "    output_df.to_excel(output_file_path, index=False)    # Saving output DataFrame to Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea25012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the entire script.\n",
    "    \"\"\"\n",
    "    file_path = 'F:/Blackcoffer assignment/input.xlsx'    # Path to the input Excel file containing URLs\n",
    "    All_stop_words_path = 'F:/Blackcoffer assignment/StopWords-20230812T205218Z-001/StopWords/AllStopWords.txt'    # Path to the file containing stop words\n",
    "    positive_words_file = 'F:/Blackcoffer assignment/MasterDictionary-20230812T205219Z-001/MasterDictionary/positive-words.txt'    # Path to the file containing positive words\n",
    "    negative_words_file = 'F:/Blackcoffer assignment/MasterDictionary-20230812T205219Z-001/MasterDictionary/negative-words.txt'    # Path to the file containing negative words\n",
    "    output_file = 'F:/Blackcoffer assignment/output.xlsx'    # Path to save the output Excel file\n",
    "    \n",
    "    process_urls(file_path, output_file, All_stop_words_path, positive_words, negative_words)    # Calling process_urls function with specified arguments\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()    # Calling main function to execute the entire script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a6dfcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e21a37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
